1)Недостатком one-hot encoding является то, что по векторным представлениям нельзя судить о схожести смысла слов. 
Также вектора имеют очень большой размер, из-за чего их неэффективно хранить в памяти. Для более оптимального представления используется 
word-to-vec. 
2) Вместо умножения на всю матрицу с одной единицей можно найти индекс ненулевого элемента
 и взять соответсвующий столбец у левой матрицы. Таким образом, можно немного ускорить работуобучение нейронной сети
3) Использование Softmax между линейными слоями даёт небольшой выигрыш в качествеи предсказания контекста
(5,34 против 5,36) и позволяет немного четче выделить признаки в векторном представлении слов матрицей. 
Для повышения точности можно использовать Hierarchical Softmax и Negative Sampling.
Hierarchical Softmax полезнее при работе с не очень частотными словами. 
Negative sampling лучше работает с частотными и вкторами слов небольшой размерности, но при этом часто быстрее Hierarchical Softmax
